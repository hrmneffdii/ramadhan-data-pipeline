# ğŸ“Š Ramadhan Seasonal Data Pipeline (ETL)

End-to-end data engineering project to analyze seasonal commodity trends during the Ramadhan period using Google Trends data.  
This project simulates a real-world ETL pipeline built with a modular structure and reproducible environment.

## ğŸš€ Project Overview
This pipeline extracts search trend data (e.g., "takjil", "baju lebaran", "snack lebaran") to identify hype movement of commodities during the Ramadhan season in Indonesia.

The goal of this project:
- Practice real-world ETL pipeline design
- Build a data engineering portfolio project
- Analyze seasonal demand signals for commodities

## ğŸ—ï¸ Project Structure
```

ramadhan-data-pipeline/
â”œâ”€â”€ etl/            # Extract logic (Google Trends)
â”œâ”€â”€ config/         # Configuration files
â”œâ”€â”€ data/           # Raw & processed data (ignored in git)
â”œâ”€â”€ logs/           # Pipeline logs
â”œâ”€â”€ main.py         # Pipeline entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

````

## âš™ï¸ Tech Stack
- Python 3.12
- Pandas
- Pytrends (Google Trends API)
- Ubuntu (local development)
- Git & GitHub (version control)

## ğŸ”„ Pipeline Architecture
ETL Flow:
1. Extract â†’ Fetch Google Trends data (Indonesia, last 3 months)
2. Transform â†’ Clean & structure time-series data
3. Load â†’ Save into local storage (CSV / future warehouse)

## ğŸ“¥ Data Source
- Google Trends (via Pytrends)
- Keywords example:
  - Takjil
  - Baju Lebaran
  - Snack Lebaran

## ğŸ› ï¸ Setup & Installation
Clone the repository:
```bash
git clone https://github.com/YOUR_USERNAME/ramadhan-data-pipeline.git
cd ramadhan-data-pipeline
````

Create virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Run the Pipeline

```bash
python etl/extract_trends.py
```

Output will be saved in:

```
data/trends_raw.csv
```

## ğŸ“Œ Notes

* `venv/`, `data/`, and `logs/` are excluded from Git for best practices
* Designed to be scalable to cloud (AWS/GCP) in future iterations
* Built on a low-spec machine (Core i3, 12GB RAM) to simulate realistic constraints

## ğŸ¯ Future Improvements

* Add data warehouse (SQLite / PostgreSQL)
* Orchestrate with Airflow
* Deploy pipeline to AWS (S3 + Lambda + Glue)
* Dashboard visualization (Power BI / Streamlit)
